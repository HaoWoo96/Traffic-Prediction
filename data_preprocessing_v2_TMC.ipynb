{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Version 2 TMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Differences from Data Preprocessing Version 1:\n",
    "- Input:\n",
    "    - Include XD speed data (aggregated in the frequency of 5 min)  \n",
    "- Output Ground Truth\n",
    "    - Incident Indicator: include segments whose speed is abnormal\n",
    "    - another output ground truth whose speed data refers to 3 types of TMC speed data (All, Truck only, Personal Vehicle only), rather than XD data => we will have two output ground truth files (new_Y_TMC.npy and new_Y_XD.npy)\n",
    "\n",
    "Version 2 doesn't make change to Part 1. Segment Selection for New Input & Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Segment Selection for New Input & Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Columns:\n",
    "    'tmc', 'road', 'direction', 'intersection', 'state', 'county', 'zip',\n",
    "    'start_latitude', 'start_longitude', 'end_latitude', 'end_longitude',\n",
    "    'miles', 'road_order', 'timezone_name', 'type', 'country', 'tmclinear',\n",
    "    'frc', 'border_set', 'f_system', 'urban_code', 'faciltype', 'structype',\n",
    "    'thrulanes', 'route_numb', 'route_sign', 'route_qual', 'altrtename',\n",
    "    'aadt', 'aadt_singl', 'aadt_combi', 'nhs', 'nhs_pct', 'strhnt_typ',\n",
    "    'strhnt_pct', 'truck', 'isprimary', 'active_start_date',\n",
    "    'active_end_date'\n",
    "'''\n",
    "tmc = pd.read_csv(\"data/Carnberry_NPMRDS_5min/manually_select_cranberry_2019_dont_average_2/TMC_Identification.csv\")  # (1248, 39), this is the comprehensive list, TMC speed csv data (All/Truck/PV) may not include all TMC in this list\n",
    "tmc_coord = tmc.loc[:, [\"tmc\", 'start_latitude', 'start_longitude', 'end_latitude', 'end_longitude']]\n",
    "\n",
    "'''\n",
    "Columns:\n",
    "    'xd', 'road-name', 'road-num', 'bearing', 'miles', 'frc', 'county',\n",
    "    'state', 'zip', 'timezone_name', 'start_latitude', 'start_longitude',\n",
    "    'end_latitude', 'end_longitude'\n",
    "'''\n",
    "xd = pd.read_csv(\"data/Cranberry_ritis_1min_class123/manually_select_cranberry_class123_20181101_20190727_dont_average/XD_Identification.csv\")  # (1628, 14)\n",
    "xd[\"xd\"] = xd[\"xd\"].apply(str)\n",
    "xd_coord = xd.loc[:, [\"xd\", 'start_latitude', 'start_longitude', 'end_latitude', 'end_longitude']]\n",
    "\n",
    "'''\n",
    "162 targeted IDs\n",
    "    TMC: 129 (84 found in tmc)\n",
    "    XD: 33 (21 found in xd)\n",
    "'''\n",
    "old_out = list(np.load(\"data/cran_tmc.npy\", allow_pickle=True))\n",
    "old_out_tmc = old_out[:129]\n",
    "old_out_xd = old_out[130:]\n",
    "\n",
    "\n",
    "col_names = list(np.load(\"data/col_names.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Select Targeted Segments (Input & Output) by Referring to Old Targeted Segments (Input & Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_in_seg = {}\n",
    "old_in_seg[\"tti\"] = {} # 369 (331 tmc + 38 xd), including all 162 segments in old_target\n",
    "old_in_seg[\"tti\"][\"tmc\"] = [i for i in col_names[:-21] if \"sd\" not in i and \"inc\" not in i and i.startswith(\"104\")]\n",
    "old_in_seg[\"tti\"][\"xd\"] = [i for i in col_names[:-21] if \"sd\" not in i and \"inc\" not in i and not i.startswith(\"104\")]\n",
    "old_in_seg[\"inc\"] = [i[4:] for i in col_names[:-21] if \"inc\" in i] # 315 (275 tmc + 40 xd), has 309 in common with old_in_seg[\"tti\"]\n",
    "old_in_seg[\"sd\"] = [i[3:] for i in col_names[:-21] if \"sd\" in i] # 303 (266 tmc + 37 xd), subset of old_in_seg[\"inc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_tmc = set(tmc[tmc[\"tmc\"].isin(old_out)][\"tmc\"])  # 84 preliminarily selected TMC segments for output\n",
    "new_out_xd = set(xd[xd[\"xd\"].isin(old_out)][\"xd\"])  # 21 preliminarily selected XD segments for output\n",
    "\n",
    "new_in_seg = {}\n",
    "new_in_seg[\"tti\"] = {}\n",
    "new_in_seg[\"tti\"][\"tmc\"] = list(tmc[tmc[\"tmc\"].isin(old_in_seg[\"tti\"][\"tmc\"])][\"tmc\"])  # 236\n",
    "new_in_seg[\"tti\"][\"xd\"] =list(xd[xd[\"xd\"].isin(old_in_seg[\"tti\"][\"xd\"])][\"xd\"])  # 21\n",
    "new_in_seg[\"inc\"] = {}\n",
    "new_in_seg[\"inc\"][\"tmc\"] = list(tmc[tmc[\"tmc\"].isin(old_in_seg[\"inc\"])][\"tmc\"])  # 195, including all new_out_tmc\n",
    "new_in_seg[\"inc\"][\"xd\"] =list(xd[xd[\"xd\"].isin(old_in_seg[\"inc\"])][\"xd\"])  # 21, including all new_out_xd\n",
    "new_in_seg[\"sd\"] = {}\n",
    "new_in_seg[\"sd\"][\"tmc\"] = list(tmc[tmc[\"tmc\"].isin(old_in_seg[\"sd\"])][\"tmc\"])  # 190\n",
    "new_in_seg[\"sd\"][\"xd\"] =list(xd[xd[\"xd\"].isin(old_in_seg[\"sd\"])][\"xd\"])  # 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_in_tmc = set(new_in_seg[\"tti\"][\"tmc\"] + new_in_seg[\"sd\"][\"tmc\"] + new_in_seg[\"inc\"][\"tmc\"])  # 236\n",
    "new_in_xd = set(new_in_seg[\"tti\"][\"xd\"] + new_in_seg[\"sd\"][\"xd\"] + new_in_seg[\"inc\"][\"xd\"])  # 21\n",
    "new_in = new_in_tmc.union(new_in_xd)  # 257 targeted TMC & XD segments for input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Match TMCs and XDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pairwise distance between TMCs and XDs on their starting & ending coordinates\n",
    "pairwise_dist_all = cdist(tmc_coord.iloc[:, 1:], xd_coord.iloc[:, 1:], metric=\"euclidean\")  # (1248, 1628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.1 Match TMC with XD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some XDs may be mapped to multiple TMCs\n",
    "match_xd_cnt = Counter(pairwise_dist_all.argmin(1))  # 426 XDs that achieve one-to-one match with TMCs\n",
    "dup_xd = [xd.loc[i, \"xd\"] for i in match_xd_cnt.keys() if match_xd_cnt[i] > 1]  # account for 822 TMCs\n",
    "\n",
    "match_tmc_to_xd = tmc.copy()\n",
    "match_tmc_to_xd[\"direction\"] = match_tmc_to_xd[\"direction\"].apply(lambda x: x[0])  # convert direction values from \"South\" to \"S\", etc\n",
    "match_tmc_to_xd = match_tmc_to_xd.iloc[:, :-14]  # remove auxiliary info\n",
    "match_tmc_to_xd.drop([\"state\", \"country\", \"timezone_name\"], axis=1, inplace=True)\n",
    "\n",
    "# One-to-one match TMCs with XDs\n",
    "match_tmc_to_xd[\"xd_dist\"] = pairwise_dist_all.min(1)\n",
    "match_tmc_to_xd[\"xd\"] = (pd.Series(pairwise_dist_all.argmin(1))).apply(lambda x: xd.loc[x, \"xd\"])\n",
    "# match_tmc_to_xd = match_tmc_to_xd[~match_tmc_to_xd[\"xd\"].isin(dup_xd)]  # here we don't remove XDs that correspond to multiple TMCs\n",
    "\n",
    "# Merge Dataframes\n",
    "match_tmc_to_xd = match_tmc_to_xd.merge(xd, on=\"xd\")\n",
    "\n",
    "# remove XDs that has different directions from TMCs\n",
    "match_tmc_to_xd = match_tmc_to_xd[(match_tmc_to_xd[\"direction\"] == match_tmc_to_xd[\"bearing\"]) | (match_tmc_to_xd[\"xd_dist\"] ==0)]  # 353 TMCs\n",
    "match_tmc_to_xd.to_csv(\"data/temp_match_tmc_xd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2 Match XD with TMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_tmc_cnt = Counter(pairwise_dist_all.argmin(0))  # 308 TMCs achieves one-to-one match with XDs\n",
    "dup_tmc = [tmc.loc[i, \"tmc\"] for i in match_tmc_cnt.keys() if match_tmc_cnt[i] > 1]  # 301 TMCs account for the remaining 1320 XDs\n",
    "\n",
    "# match XDs with TMCs\n",
    "match_xd_to_tmc = xd.copy()\n",
    "match_xd_to_tmc[\"tmc_dist\"] = pairwise_dist_all.min(0)\n",
    "match_xd_to_tmc[\"tmc\"] = (pd.Series(pairwise_dist_all.argmin(0))).apply(lambda x: tmc.loc[x, \"tmc\"])\n",
    "# match_xd_to_tmc = match_xd_to_tmc[~match_xd_to_tmc[\"tmc\"].isin(dup_tmc)]  # here we don't remove TMCs that correspond to multiple XDs\n",
    "\n",
    "# Merge Dataframes\n",
    "match_xd_to_tmc = match_xd_to_tmc.merge(tmc, on=\"tmc\")\n",
    "match_xd_to_tmc[\"direction\"] = match_xd_to_tmc[\"direction\"].apply(lambda x: x[0])\n",
    "\n",
    "# remove XDs that has different directions from TMCs\n",
    "match_xd_to_tmc = match_xd_to_tmc[(match_xd_to_tmc[\"direction\"] == match_xd_to_tmc[\"bearing\"]) | (match_xd_to_tmc[\"tmc_dist\"] ==0)]  \n",
    "match_xd_to_tmc.to_csv(\"data/temp_match_xd_to_tmc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Get Final Targeted TMC & XD segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tmc = match_tmc_to_xd[match_tmc_to_xd[\"tmc\"].isin(new_out_tmc)]  # 60 (whose XDs are different from target_xd)\n",
    "target_xd = match_xd_to_tmc[match_xd_to_tmc[\"xd\"].isin(new_out_xd)]  # 10\n",
    "\n",
    "new_out_tmc = set(target_tmc[\"tmc\"])  # 60 eventually targeted TMC segments for output, is a subset of new_in\n",
    "new_out_xd = set(target_xd[\"xd\"])  # 10 eventually targeted XD segments for output, is a subset of new_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tmc.to_csv(\"data/temp_match_target_tmc_to_xd.csv\", index=False)\n",
    "target_xd.to_csv(\"data/temp_match_target_xd_to_tmc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_tmc_xd = new_out_tmc.union(new_out_xd)\n",
    "\n",
    "# new_out_all_in_xd = set(target_tmc[\"xd\"]).union(new_out_xd)  # 69 unique XD id of 70 targeted TMC & XD segments for output\n",
    "# new_out_all_in_xd_int = set([int(i) for i in list(new_out_all_in_xd)])\n",
    "new_out_all_in_xd = list(target_tmc[\"xd\"]) + list(new_out_xd)  # 69 unique XD id of 70 targeted TMC & XD segments for output\n",
    "new_out_all_in_xd_int = [int(i) for i in list(new_out_all_in_xd)]\n",
    "\n",
    "# new_out_all_in_tmc = set(target_xd[\"tmc\"]).union(new_out_tmc)  # 63 unique TMC id of 70 targeted TMC & XD segments for output\n",
    "new_out_all_in_tmc = list(new_out_tmc) + list(target_xd[\"tmc\"])  # 63 unique TMC id of 70 targeted TMC & XD segments for output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Generate New Input & Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Date: 2019.2.10 ~ 2019.7.23 (164 days, including all holidays & weekends)\n",
    "Time Slots: \n",
    "    - For each day, there are 180 targeted time slots from 06:00:00 to 20:55:00\n",
    "    - For each targeted time slot t (t in 06:00:00 ~ 20:55:00), \n",
    "        - old_Y contains padding of 7 slots (t-6, t-5, t-4, t-3, t-2, t-1, t)\n",
    "        - old_X contains padding of 7 slots as input (t-12, t-11, t-10, t-9, t-8, t-7, t-6)\n",
    "    \n",
    "    In new_X and new Y, to allow for more flexibility of hyperparameters and reduce the file size, there won't be padding\n",
    "    For example, for targeted time slot 06:00:00\n",
    "        - old_Y has 05:30:00, 05:35:00, 05:40:00, 05:45:00, 05:50:00, 05:55:00, 06:00:00\n",
    "        - old_X has 05:00:00, 05:05:00, 05:10:00, 05:15:00, 05:20:00, 05:25:00, 05:30:00\n",
    "        - new_Y has 06:00:00\n",
    "        - new_X has 05:30:00\n",
    "'''\n",
    "old_X = np.load(\"data/X.npy\")  # (29520, 7, 1008)\n",
    "old_Y = np.load(\"data/Y.npy\")  # (29520, 7, 162)\n",
    "old_col = list(np.load(\"data/col_names.npy\", allow_pickle=True))  # 1008 (369 tti, 315 inc, 303 sd, 21 weather & time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of old_col that will remain as new columns\n",
    "col_idx = []  # will store the indices of 704 old columns that will become new_X (257 tti, 210 sd, 216 inc, 21 weather & time)\n",
    "for i in range(987):\n",
    "    c = old_col[i]\n",
    "    if \"sd\" in c:\n",
    "        c = c[3:]\n",
    "    if \"inc\" in c:\n",
    "        c = c[4:]\n",
    "    \n",
    "    if c in new_in:\n",
    "        col_idx.append(i)\n",
    "col_idx += list(range(987, 1008))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Generate New Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_X without additional features\n",
    "new_X = old_X[:, -1, col_idx] # (29520, 704) 164 days * 180 daily time slots (05:30:00 ~ 20:25:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Columns:\n",
    "    'tmc_code', 'measurement_tstamp', 'speed', 'average_speed',\n",
    "    'reference_speed', 'travel_time_minutes', 'data_density'\n",
    "'''\n",
    "tmc_truck = pd.read_csv(\"data/Carnberry_NPMRDS_5min/manually_select_cranberry_2019_dont_average/manually_select_cranberry_2019_dont_average.csv\") # 9136192, 7\n",
    "tmc_pv = pd.read_csv(\"data/Carnberry_NPMRDS_5min/manually_select_cranberry_2019_dont_average_3/manually_select_cranberry_2019_dont_average.csv\")  # 21385940, 7\n",
    "tmc_all = pd.read_csv(\"data/Carnberry_NPMRDS_5min/manually_select_cranberry_2019_dont_average_2/manually_select_cranberry_2019_dont_average.csv\") # 24388983,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed of all vehicles\n",
    "all_spd = tmc_all.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"speed\"]]\n",
    "all_spd = all_spd.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"speed\")\n",
    "\n",
    "# select 233 tmc segments based on old tmc input segments\n",
    "all_spd = all_spd.loc[:, [c for c in all_spd.columns if c in new_in_tmc]]  \n",
    "\n",
    "# convert index to datetime object, and select 29520 rows of interest\n",
    "all_spd.index = pd.to_datetime(all_spd.index)\n",
    "all_spd = all_spd.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "all_spd = all_spd[(all_spd.index.hour * 60 + all_spd.index.minute >= 330 ) & (all_spd.index.hour * 60 + all_spd.index.minute <= 1225)] # 1858961 NaN\n",
    "\n",
    "all_avg = tmc_all.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"average_speed\"]]\n",
    "all_avg = all_avg.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"average_speed\")\n",
    "all_avg = all_avg.loc[:, [c for c in all_avg.columns if c in new_in_tmc]]  \n",
    "all_avg.index = pd.to_datetime(all_avg.index)\n",
    "all_avg = all_avg.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "all_avg = all_avg[(all_avg.index.hour * 60 + all_avg.index.minute >= 330 ) & (all_avg.index.hour * 60 + all_avg.index.minute <= 1225)] # 1859011 NaN\n",
    "\n",
    "all_ref = tmc_all.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"reference_speed\"]]\n",
    "all_ref = all_ref.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"reference_speed\")\n",
    "all_ref = all_ref.loc[:, [c for c in all_ref.columns if c in new_in_tmc]] \n",
    "all_ref.index = pd.to_datetime(all_ref.index)\n",
    "all_ref = all_ref.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "all_ref = all_ref[(all_ref.index.hour * 60 + all_ref.index.minute >= 330 ) & (all_ref.index.hour * 60 + all_ref.index.minute <= 1225)] # 1858961 NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1 Prepare Density Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_tmc = tmc_all.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"data_density\"]]\n",
    "density_tmc = density_tmc.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"data_density\")\n",
    "\n",
    "# select 233 tmc segments based on old tmc input segments. 233 tmc segments include all new_out_all_in_tmc\n",
    "density_tmc = density_tmc.loc[:, [c for c in density_tmc.columns if c in new_in_tmc]] \n",
    "\n",
    "# fill NAN with \"A\", which denotes \"Fewer than five values\"\n",
    "density_tmc = density_tmc.fillna(\"A\")\n",
    "\n",
    "# convert index to datetime object, and select 29520 rows of interest\n",
    "density_tmc.index = pd.to_datetime(density_tmc.index)\n",
    "density_tmc = density_tmc.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "density_tmc = density_tmc[(density_tmc.index.hour * 60 + density_tmc.index.minute >= 330 ) & (density_tmc.index.hour * 60 + density_tmc.index.minute <= 1225)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233, 233)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([c for c in density_tmc.columns if c in old_in_seg[\"tti\"][\"tmc\"]]), len([c for c in density_tmc.columns if c in new_in_tmc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Prepare Features of Truck Speed & Personal Vehicle Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: prepare 1. TMC All spd featuer & 2. XD spd feature (aggregated in 5 min) for new_in_tmc & new_in_xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "truck = tmc_truck.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"speed\"]]\n",
    "truck = truck.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"speed\")\n",
    "\n",
    "# select 233 tmc segments based on old tmc input segments\n",
    "truck = truck.loc[:, [c for c in truck.columns if c in new_in_tmc]]  \n",
    "\n",
    "# convert index to datetime object, and select 29520 rows of interest\n",
    "truck.index = pd.to_datetime(truck.index)\n",
    "truck = truck.resample(\"5 min\").asfreq()  # upsampling with 5-min frequency\n",
    "truck = truck.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "truck = truck[(truck.index.hour * 60 + truck.index.minute >= 330 ) & (truck.index.hour * 60 + truck.index.minute <= 1225)] # 4461384 NaN\n",
    "\n",
    "truck_avg = tmc_truck.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"average_speed\"]]\n",
    "truck_avg = truck_avg.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"average_speed\")\n",
    "truck_avg = truck_avg.loc[:, [c for c in truck_avg.columns if c in new_in_tmc]]  \n",
    "truck_avg.index = pd.to_datetime(truck_avg.index)\n",
    "truck_avg = truck_avg.resample(\"5 min\").asfreq()\n",
    "truck_avg = truck_avg.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "truck_avg = truck_avg[(truck_avg.index.hour * 60 + truck_avg.index.minute >= 330 ) & (truck_avg.index.hour * 60 + truck_avg.index.minute <= 1225)]  # 4461384 NaN\n",
    "\n",
    "truck_ref = tmc_truck.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"reference_speed\"]]\n",
    "truck_ref = truck_ref.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"reference_speed\")\n",
    "truck_ref = truck_ref.loc[:, [c for c in truck_ref.columns if c in new_in_tmc]] \n",
    "truck_ref.index = pd.to_datetime(truck_ref.index)\n",
    "truck_ref = truck_ref.resample(\"5 min\").asfreq()\n",
    "truck_ref = truck_ref.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "truck_ref = truck_ref[(truck_ref.index.hour * 60 + truck_ref.index.minute >= 330 ) & (truck_ref.index.hour * 60 + truck_ref.index.minute <= 1225)]  # 4461384 NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = tmc_pv.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"speed\"]]\n",
    "pv = pv.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"speed\")\n",
    "\n",
    "# select 233 tmc segments based on old tmc input segments\n",
    "pv = pv.loc[:, [c for c in pv.columns if c in new_in_tmc]] \n",
    "\n",
    "# convert index to datetime object, and select 29520 rows of interest\n",
    "pv.index = pd.to_datetime(pv.index)\n",
    "pv = pv.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "pv = pv[(pv.index.hour * 60 + pv.index.minute >= 330 ) & (pv.index.hour * 60 + pv.index.minute <= 1225)]  # 2258550 NaN\n",
    "\n",
    "pv_avg = tmc_pv.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"average_speed\"]]\n",
    "pv_avg = pv_avg.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"average_speed\")\n",
    "pv_avg = pv_avg.loc[:, [c for c in pv_avg.columns if c in new_in_tmc]]  \n",
    "pv_avg.index = pd.to_datetime(pv_avg.index)\n",
    "pv_avg = pv_avg.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "pv_avg = pv_avg[(pv_avg.index.hour * 60 + pv_avg.index.minute >= 330 ) & (pv_avg.index.hour * 60 + pv_avg.index.minute <= 1225)]  # 2258600 NaN\n",
    "\n",
    "pv_ref = tmc_pv.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"reference_speed\"]]\n",
    "pv_ref = pv_ref.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"reference_speed\")\n",
    "pv_ref = pv_ref.loc[:, [c for c in pv_ref.columns if c in new_in_tmc]]  \n",
    "pv_ref.index = pd.to_datetime(pv_ref.index)\n",
    "pv_ref = pv_ref.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "pv_ref = pv_ref[(pv_ref.index.hour * 60 + pv_ref.index.minute >= 330 ) & (pv_ref.index.hour * 60 + pv_ref.index.minute <= 1225)]  # 2258550 NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with historical and reference speed data\n",
    "truck = truck.fillna(truck_avg)\n",
    "truck = truck.fillna(truck_ref)\n",
    "\n",
    "pv = pv.fillna(pv_avg)\n",
    "pv = pv.fillna(pv_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_seg_avg = truck.mean(axis=0, skipna=True)\n",
    "all_seg_avg = all_spd.mean(axis=0, skipna=True)\n",
    "pv_seg_avg = pv.mean(axis=0, skipna=True)\n",
    "\n",
    "# compute fillna weight\n",
    "truck_vs_all = truck_seg_avg / all_seg_avg\n",
    "truck_vs_pv = truck_seg_avg / pv_seg_avg\n",
    "pv_vs_all = pv_seg_avg / all_seg_avg\n",
    "pv_vs_truck = pv_seg_avg / truck_seg_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted fillna with all speed data and private vehicle speed data\n",
    "truck = truck.fillna(all_spd * truck_vs_all)  # reduce NAN from 4461384 to 1858961\n",
    "truck = truck.fillna(all_avg * truck_vs_all)  \n",
    "truck = truck.fillna(all_ref * truck_vs_all)  \n",
    "truck = truck.fillna(pv * truck_vs_pv)  \n",
    "truck = truck.fillna(pv_avg * truck_vs_pv)\n",
    "truck = truck.fillna(pv_ref * truck_vs_pv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted fillna with all speed data and truck speed data\n",
    "pv = pv.fillna(all_spd * pv_vs_all)  # reduce NaN from 2258550 to 1858961\n",
    "pv = pv.fillna(all_avg * pv_vs_all)\n",
    "pv = pv.fillna(all_ref * pv_vs_all)\n",
    "pv = pv.fillna(truck * pv_vs_truck)\n",
    "pv = pv.fillna(truck_avg * pv_vs_truck)\n",
    "pv = pv.fillna(truck_ref * pv_vs_truck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate for NAN value\n",
    "truck = truck.interpolate(method=\"linear\")\n",
    "pv = pv.interpolate(method = \"linear\")\n",
    "\n",
    "# fill the remaining NaN with column mean\n",
    "truck = truck.fillna(truck_seg_avg)\n",
    "pv = pv.fillna(pv_seg_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.3 Merge New Features into New_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Embedding for Density & Inc Features\n",
    "density = density.replace([\"A\", \"B\", \"C\"], [1/6, 3/6, 5/6])\n",
    "\n",
    "new_X[:,467:683][new_X[:,467:683] == 0.0] = 1/6\n",
    "new_X[:,467:683][new_X[:,467:683] == 1.0] = 3/6\n",
    "new_X[:,467:683][new_X[:,467:683] == 2.0] = 5/6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X = np.concatenate((density.to_numpy(), truck.to_numpy(), pv.to_numpy(), new_X), axis=1)  # (29520, 1403) (233 density, 233 truck, 233 pv, 257 tti, 210 sd, 216 inc, 21 weather & time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Normalization (min-max normalization)\n",
    "scaler = MinMaxScaler()\n",
    "final_X = scaler.fit_transform(final_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/new_X.npy\", final_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Generate New Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 Speed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: \n",
    "# 根据new_out_all_in_xd_int重新跑一遍downsample -> 获得最新的downsampled_xd\n",
    "# generate new output with XD spd (be careful, we need 70 columns, and there are duplicates because two TMCs match the same XD)\n",
    "# generate new output with TMC spd (be careful, we need 70 columns, and there are duplicates because several XDs match the same TMCs, and some of those matched TMCs are also in target_tmc)\n",
    "# reconsider incident data by incorporating speed information\n",
    "# also be careful about aligning columns of spd data with columns of incident data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [07:08,  6.92s/it]\n"
     ]
    }
   ],
   "source": [
    "start_date = dt(2019, 2, 10)\n",
    "end_date = dt(2019, 7, 24)\n",
    "\n",
    "'''\n",
    "Header:\n",
    "    'xd_id', 'measurement_tstamp', 'speed', 'average_speed', 'reference_speed', 'travel_time_minutes', 'confidence_score', 'cvalue'\n",
    "'''\n",
    "# the original csv file stores XD speed data in 1-min slots from 2018.11.1 to 2019.7.27, which is too large\n",
    "# therefore, we have to read and split csv into 61 dataframe chunks and apply operation individually \n",
    "chunksize = 10 ** 7\n",
    "xd_file = \"data/Cranberry_ritis_1min_class123/manually_select_cranberry_class123_20181101_20190727_dont_average/manually_select_cranberry_class123_20181101_20190727_dont_average.csv\"\n",
    "chunklist = []\n",
    "with pd.read_csv(xd_file, chunksize=chunksize) as reader:\n",
    "    for chunk in tqdm(reader):\n",
    "        chunk.measurement_tstamp = pd.to_datetime(chunk.measurement_tstamp)\n",
    "\n",
    "        # filter dataframe by selecting rows based on xd_id and timestamp of our interest\n",
    "        chunk = chunk[\n",
    "                (chunk.xd_id.isin(new_out_all_in_xd_int)) &\n",
    "                (start_date <= chunk.measurement_tstamp) & \n",
    "                (chunk.measurement_tstamp < end_date) & \n",
    "                (chunk.measurement_tstamp.dt.hour*60 + chunk.measurement_tstamp.dt.minute >= 360) & \n",
    "                (chunk.measurement_tstamp.dt.hour*60 + chunk.measurement_tstamp.dt.minute < 1260) \n",
    "                ]\n",
    "        chunklist.append(chunk)\n",
    "\n",
    "# concat dataframe chunks and merge into one final dataframe \n",
    "downsampled_xd = pd.concat(chunklist) \n",
    "downsampled_xd = downsampled_xd.reset_index(drop=True)  # reset index\n",
    "\n",
    "# save downsampled xd data\n",
    "downsampled_xd.to_csv(\"data/downsampled_xd_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_spd = downsampled_xd.pivot(index = \"measurement_tstamp\", columns = \"xd_id\", values = \"speed\")  # (147600, 78)\n",
    "xd_avg = downsampled_xd.pivot(index = \"measurement_tstamp\", columns = \"xd_id\", values = \"average_speed\")\n",
    "xd_ref = downsampled_xd.pivot(index = \"measurement_tstamp\", columns = \"xd_id\", values = \"reference_speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_spd = xd_spd.interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 Incident Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_inc_label = pd.read_csv(\"data/incident_labels.csv\")\n",
    "out_inc_label = out_inc_label.set_index(\"measurement_tstamp\")\n",
    "out_inc_label.index = pd.to_datetime(out_inc_label.index)\n",
    "\n",
    "out_inc_label = out_inc_label.resample(\"1 min\").asfreq()  # upsampling with 1-min frequency\n",
    "\n",
    "# select rows based on timestamps of our interest (2019.2.10 ~ 2019.7.23, 06:00:00~20:59:00 in 1-min frequency everyday)\n",
    "out_inc_label = out_inc_label[\n",
    "                (start_date <= out_inc_label.index) & \n",
    "                (out_inc_label.index < end_date) & \n",
    "                (out_inc_label.index.hour*60 + out_inc_label.index.minute >= 360) & \n",
    "                (out_inc_label.index.hour*60 + out_inc_label.index.minute < 1260) \n",
    "                ]\n",
    "\n",
    "# select columns of new out segments \n",
    "out_inc_label = out_inc_label.loc[:, new_out_tmc_xd]  # (147600, 78)\n",
    "\n",
    "# rename columns to represent all segments in XD IDs, so that we can combine incident data with xd speed data eventually\n",
    "new_col_name = [c if c in new_out_xd else target_tmc[target_tmc.tmc == c][\"xd\"].values[0] for c in out_inc_label.columns]\n",
    "out_inc_label.columns = new_col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be cautious about the order of backfilling and forward filling\n",
    "# We first do a backfilling because when an incident occurs, we want the segment to be marked \"1\" as early as possible\n",
    "# Backfilling allows NaN slots before a slot already marked \"1\" to be marked as \"1\" as well.\n",
    "# Then, we do a forward filling to fill NaN at the end of the dataframe\n",
    "out_inc_label = out_inc_label.fillna(method = \"bfill\")\n",
    "out_inc_label = out_inc_label.fillna(method = \"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.3 Merge into New_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_spd.columns = [str(c) for c in xd_spd.columns] \n",
    "\n",
    "# align the columns of xd speed dataframe and incident dataframe to make sure they have segments in the same order\n",
    "final_xd_spd, final_out_inc_label = xd_spd.align(out_inc_label, join='inner', axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Y = np.stack((final_xd_spd.to_numpy().astype(\"float64\"), final_out_inc_label.to_numpy()), axis=-1)  # (147600, 78, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/new_Y.npy\", new_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
