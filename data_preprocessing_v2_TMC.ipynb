{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Version 2 TMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Differences from Data Preprocessing Version 1:\n",
    "- Input:\n",
    "    - Include XD speed data (aggregated in the frequency of 5 min)  \n",
    "- Output Ground Truth\n",
    "    - Incident Indicator: include segments whose speed is abnormal\n",
    "    - another output ground truth whose speed data refers to 3 types of TMC speed data (All, Truck only, Personal Vehicle only), rather than XD data => we will have two output ground truth files (new_Y_TMC.npy and new_Y_XD.npy)\n",
    "\n",
    "Version 2 doesn't make change to Part 1. Segment Selection for New Input & Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Segment Selection for New Input & Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Columns:\n",
    "    'tmc', 'road', 'direction', 'intersection', 'state', 'county', 'zip',\n",
    "    'start_latitude', 'start_longitude', 'end_latitude', 'end_longitude',\n",
    "    'miles', 'road_order', 'timezone_name', 'type', 'country', 'tmclinear',\n",
    "    'frc', 'border_set', 'f_system', 'urban_code', 'faciltype', 'structype',\n",
    "    'thrulanes', 'route_numb', 'route_sign', 'route_qual', 'altrtename',\n",
    "    'aadt', 'aadt_singl', 'aadt_combi', 'nhs', 'nhs_pct', 'strhnt_typ',\n",
    "    'strhnt_pct', 'truck', 'isprimary', 'active_start_date',\n",
    "    'active_end_date'\n",
    "'''\n",
    "tmc = pd.read_csv(\"data/Carnberry_NPMRDS_5min/manually_select_cranberry_2019_dont_average_2/TMC_Identification.csv\")  # (1248, 39), this is the comprehensive list, TMC speed csv data (All/Truck/PV) may not include all TMC in this list\n",
    "tmc_coord = tmc.loc[:, [\"tmc\", 'start_latitude', 'start_longitude', 'end_latitude', 'end_longitude']]\n",
    "\n",
    "'''\n",
    "Columns:\n",
    "    'xd', 'road-name', 'road-num', 'bearing', 'miles', 'frc', 'county',\n",
    "    'state', 'zip', 'timezone_name', 'start_latitude', 'start_longitude',\n",
    "    'end_latitude', 'end_longitude'\n",
    "'''\n",
    "xd = pd.read_csv(\"data/Cranberry_ritis_1min_class123/manually_select_cranberry_class123_20181101_20190727_dont_average/XD_Identification.csv\")  # (1628, 14)\n",
    "xd[\"xd\"] = xd[\"xd\"].apply(str)\n",
    "xd_coord = xd.loc[:, [\"xd\", 'start_latitude', 'start_longitude', 'end_latitude', 'end_longitude']]\n",
    "\n",
    "'''\n",
    "162 targeted IDs\n",
    "    TMC: 129 (84 found in tmc)\n",
    "    XD: 33 (21 found in xd)\n",
    "'''\n",
    "old_out = list(np.load(\"data/cran_tmc.npy\", allow_pickle=True))\n",
    "old_out_tmc = old_out[:129]\n",
    "old_out_xd = old_out[130:]\n",
    "\n",
    "\n",
    "col_names = list(np.load(\"data/col_names.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Select Targeted Segments (Input & Output) by Referring to Old Targeted Segments (Input & Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_in_seg = {}\n",
    "old_in_seg[\"tti\"] = {} # 369 (331 tmc + 38 xd), including all 162 segments in old_target\n",
    "old_in_seg[\"tti\"][\"tmc\"] = [i for i in col_names[:-21] if \"sd\" not in i and \"inc\" not in i and i.startswith(\"104\")]\n",
    "old_in_seg[\"tti\"][\"xd\"] = [i for i in col_names[:-21] if \"sd\" not in i and \"inc\" not in i and not i.startswith(\"104\")]\n",
    "old_in_seg[\"inc\"] = [i[4:] for i in col_names[:-21] if \"inc\" in i] # 315 (275 tmc + 40 xd), has 309 in common with old_in_seg[\"tti\"]\n",
    "old_in_seg[\"sd\"] = [i[3:] for i in col_names[:-21] if \"sd\" in i] # 303 (266 tmc + 37 xd), subset of old_in_seg[\"inc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_tmc = set(tmc[tmc[\"tmc\"].isin(old_out)][\"tmc\"])  # 84 preliminarily selected TMC segments for output\n",
    "new_out_xd = set(xd[xd[\"xd\"].isin(old_out)][\"xd\"])  # 21 preliminarily selected XD segments for output\n",
    "\n",
    "new_in_seg = {}\n",
    "new_in_seg[\"tti\"] = {}\n",
    "new_in_seg[\"tti\"][\"tmc\"] = list(tmc[tmc[\"tmc\"].isin(old_in_seg[\"tti\"][\"tmc\"])][\"tmc\"])  # 236\n",
    "new_in_seg[\"tti\"][\"xd\"] =list(xd[xd[\"xd\"].isin(old_in_seg[\"tti\"][\"xd\"])][\"xd\"])  # 21\n",
    "new_in_seg[\"inc\"] = {}\n",
    "new_in_seg[\"inc\"][\"tmc\"] = list(tmc[tmc[\"tmc\"].isin(old_in_seg[\"inc\"])][\"tmc\"])  # 195, including all new_out_tmc\n",
    "new_in_seg[\"inc\"][\"xd\"] =list(xd[xd[\"xd\"].isin(old_in_seg[\"inc\"])][\"xd\"])  # 21, including all new_out_xd\n",
    "new_in_seg[\"sd\"] = {}\n",
    "new_in_seg[\"sd\"][\"tmc\"] = list(tmc[tmc[\"tmc\"].isin(old_in_seg[\"sd\"])][\"tmc\"])  # 190\n",
    "new_in_seg[\"sd\"][\"xd\"] =list(xd[xd[\"xd\"].isin(old_in_seg[\"sd\"])][\"xd\"])  # 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_in_tmc = set(new_in_seg[\"tti\"][\"tmc\"] + new_in_seg[\"sd\"][\"tmc\"] + new_in_seg[\"inc\"][\"tmc\"])  # 236\n",
    "new_in_xd = set(new_in_seg[\"tti\"][\"xd\"] + new_in_seg[\"sd\"][\"xd\"] + new_in_seg[\"inc\"][\"xd\"])  # 21\n",
    "new_in = new_in_tmc.union(new_in_xd)  # 257 targeted TMC & XD segments for input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Match TMCs and XDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pairwise distance between TMCs and XDs on their starting & ending coordinates\n",
    "pairwise_dist_all = cdist(tmc_coord.iloc[:, 1:], xd_coord.iloc[:, 1:], metric=\"euclidean\")  # (1248, 1628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Match TMC with XD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some XDs may be mapped to multiple TMCs\n",
    "match_xd_cnt = Counter(pairwise_dist_all.argmin(1))  # 426 XDs that achieve one-to-one match with TMCs\n",
    "dup_xd = [xd.loc[i, \"xd\"] for i in match_xd_cnt.keys() if match_xd_cnt[i] > 1]  # account for 822 TMCs\n",
    "\n",
    "match_tmc_to_xd = tmc.copy()\n",
    "match_tmc_to_xd[\"direction\"] = match_tmc_to_xd[\"direction\"].apply(lambda x: x[0])  # convert direction values from \"South\" to \"S\", etc\n",
    "match_tmc_to_xd = match_tmc_to_xd.iloc[:, :-14]  # remove auxiliary info\n",
    "match_tmc_to_xd.drop([\"state\", \"country\", \"timezone_name\"], axis=1, inplace=True)\n",
    "\n",
    "# One-to-one match TMCs with XDs\n",
    "match_tmc_to_xd[\"xd_dist\"] = pairwise_dist_all.min(1)\n",
    "match_tmc_to_xd[\"xd\"] = (pd.Series(pairwise_dist_all.argmin(1))).apply(lambda x: xd.loc[x, \"xd\"])\n",
    "# match_tmc_to_xd = match_tmc_to_xd[~match_tmc_to_xd[\"xd\"].isin(dup_xd)]  # here we don't remove XDs that correspond to multiple TMCs\n",
    "\n",
    "# Merge Dataframes\n",
    "match_tmc_to_xd = match_tmc_to_xd.merge(xd, on=\"xd\")\n",
    "\n",
    "# remove XDs that has different directions from TMCs\n",
    "match_tmc_to_xd = match_tmc_to_xd[(match_tmc_to_xd[\"direction\"] == match_tmc_to_xd[\"bearing\"]) | (match_tmc_to_xd[\"xd_dist\"] ==0)]  # 353 TMCs\n",
    "match_tmc_to_xd.to_csv(\"data/temp_match_tmc_xd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Match XD with TMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_tmc_cnt = Counter(pairwise_dist_all.argmin(0))  # 308 TMCs achieves one-to-one match with XDs\n",
    "dup_tmc = [tmc.loc[i, \"tmc\"] for i in match_tmc_cnt.keys() if match_tmc_cnt[i] > 1]  # 301 TMCs account for the remaining 1320 XDs\n",
    "\n",
    "# match XDs with TMCs\n",
    "match_xd_to_tmc = xd.copy()\n",
    "match_xd_to_tmc[\"tmc_dist\"] = pairwise_dist_all.min(0)\n",
    "match_xd_to_tmc[\"tmc\"] = (pd.Series(pairwise_dist_all.argmin(0))).apply(lambda x: tmc.loc[x, \"tmc\"])\n",
    "# match_xd_to_tmc = match_xd_to_tmc[~match_xd_to_tmc[\"tmc\"].isin(dup_tmc)]  # here we don't remove TMCs that correspond to multiple XDs\n",
    "\n",
    "# Merge Dataframes\n",
    "match_xd_to_tmc = match_xd_to_tmc.merge(tmc, on=\"tmc\")\n",
    "match_xd_to_tmc[\"direction\"] = match_xd_to_tmc[\"direction\"].apply(lambda x: x[0])\n",
    "\n",
    "# remove XDs that has different directions from TMCs\n",
    "match_xd_to_tmc = match_xd_to_tmc[(match_xd_to_tmc[\"direction\"] == match_xd_to_tmc[\"bearing\"]) | (match_xd_to_tmc[\"tmc_dist\"] ==0)]  \n",
    "match_xd_to_tmc.to_csv(\"data/temp_match_xd_to_tmc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get Final Targeted TMC & XD segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tmc = match_tmc_to_xd[match_tmc_to_xd[\"tmc\"].isin(new_out_tmc)]  # 60 (whose XDs are different from target_xd)\n",
    "target_xd = match_xd_to_tmc[match_xd_to_tmc[\"xd\"].isin(new_out_xd)]  # 10 (whose TMCs are overlapping with target_tmc)\n",
    "\n",
    "new_out_tmc = list(target_tmc[\"tmc\"])  # 60 eventually targeted TMC segments for output, is a subset of new_in\n",
    "new_out_xd = list(target_xd[\"xd\"])  # 10 eventually targeted XD segments for output, is a subset of new_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tmc.to_csv(\"data/temp_match_target_tmc_to_xd.csv\", index=False)\n",
    "target_xd.to_csv(\"data/temp_match_target_xd_to_tmc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_out_tmc_xd = set(new_out_tmc).union(set(new_out_xd))\n",
    "new_out_tmc_xd = new_out_tmc + new_out_xd \n",
    "\n",
    "# new_out_all_in_xd = set(target_tmc[\"xd\"]).union(new_out_xd)  # 69 unique XD id of 70 targeted TMC & XD segments for output\n",
    "# new_out_all_in_xd_int = set([int(i) for i in list(new_out_all_in_xd)])\n",
    "new_out_all_in_xd = list(target_tmc[\"xd\"]) + new_out_xd  # 69 unique XD id of 70 targeted TMC & XD segments for output\n",
    "new_out_all_in_xd_int = [int(i) for i in list(new_out_all_in_xd)]\n",
    "\n",
    "# new_out_all_in_tmc = set(target_xd[\"tmc\"]).union(new_out_tmc)  # 63 unique TMC id of 70 targeted TMC & XD segments for output\n",
    "new_out_all_in_tmc = new_out_tmc + list(target_xd[\"tmc\"])  # 63 unique TMC id of 70 targeted TMC & XD segments for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update input segment selection to include all output segments\n",
    "new_in_tmc = new_in_tmc.union(set(new_out_all_in_tmc))  # 236 \n",
    "new_in_xd = new_in_xd.union(set(new_out_all_in_xd))  # 80\n",
    "new_in_xd_int = set([int(i) for i in list(new_in_xd)])  # 80\n",
    "new_in = new_in_tmc.union(new_in_xd)  # 316 targeted TMC & XD segments for input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Generate New Input & Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Load Old X and Old Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Date: 2019.2.10 ~ 2019.7.23 (164 days, including all holidays & weekends)\n",
    "Time Slots: \n",
    "    - For each day, there are 180 targeted time slots from 06:00:00 to 20:55:00\n",
    "    - For each targeted time slot t (t in 06:00:00 ~ 20:55:00), \n",
    "        - old_Y contains padding of 7 slots (t-6, t-5, t-4, t-3, t-2, t-1, t)\n",
    "        - old_X contains padding of 7 slots as input (t-12, t-11, t-10, t-9, t-8, t-7, t-6)\n",
    "    \n",
    "    In new_X and new Y, to allow for more flexibility of hyperparameters and reduce the file size, there won't be padding\n",
    "    For example, for targeted time slot 06:00:00\n",
    "        - old_Y has 05:30:00, 05:35:00, 05:40:00, 05:45:00, 05:50:00, 05:55:00, 06:00:00\n",
    "        - old_X has 05:00:00, 05:05:00, 05:10:00, 05:15:00, 05:20:00, 05:25:00, 05:30:00\n",
    "        - new_Y has 06:00:00\n",
    "        - new_X has 05:30:00\n",
    "'''\n",
    "old_X = np.load(\"data/X.npy\")  # (29520, 7, 1008)\n",
    "old_Y = np.load(\"data/Y.npy\")  # (29520, 7, 162)\n",
    "old_col = list(np.load(\"data/col_names.npy\", allow_pickle=True))  # 1008 (369 tti, 315 inc, 303 sd, 21 weather & time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Load TMC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Columns:\n",
    "    'tmc_code', 'measurement_tstamp', 'speed', 'average_speed',\n",
    "    'reference_speed', 'travel_time_minutes', 'data_density'\n",
    "'''\n",
    "tmc_truck = pd.read_csv(\"data/Carnberry_NPMRDS_5min/manually_select_cranberry_2019_dont_average/manually_select_cranberry_2019_dont_average.csv\") # 9136192, 7\n",
    "tmc_pv = pd.read_csv(\"data/Carnberry_NPMRDS_5min/manually_select_cranberry_2019_dont_average_3/manually_select_cranberry_2019_dont_average.csv\")  # 21385940, 7\n",
    "tmc_all = pd.read_csv(\"data/Carnberry_NPMRDS_5min/manually_select_cranberry_2019_dont_average_2/manually_select_cranberry_2019_dont_average.csv\") # 24388983,7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Load XD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [06:54,  6.69s/it]\n"
     ]
    }
   ],
   "source": [
    "start_date = dt(2019, 2, 10)\n",
    "end_date = dt(2019, 7, 24)\n",
    "\n",
    "'''\n",
    "Header:\n",
    "    'xd_id', 'measurement_tstamp', 'speed', 'average_speed', 'reference_speed', 'travel_time_minutes', 'confidence_score', 'cvalue'\n",
    "'''\n",
    "# the original csv file stores XD speed data in 1-min slots from 2018.11.1 to 2019.7.27, which is too large\n",
    "# therefore, we have to read and split csv into 61 dataframe chunks and apply operation individually \n",
    "chunksize = 10 ** 7\n",
    "xd_file = \"data/Cranberry_ritis_1min_class123/manually_select_cranberry_class123_20181101_20190727_dont_average/manually_select_cranberry_class123_20181101_20190727_dont_average.csv\"\n",
    "chunklist = []\n",
    "with pd.read_csv(xd_file, chunksize=chunksize) as reader:\n",
    "    for chunk in tqdm(reader):\n",
    "        chunk.measurement_tstamp = pd.to_datetime(chunk.measurement_tstamp)\n",
    "\n",
    "        # filter dataframe by selecting rows based on xd_id and timestamp of our interest\n",
    "        # here we select chunk from 05:30:00 to 20:59:00 to accomodate time range for both input feature (05:30:00 - 20:25:00, 5-min frequency) and output ground truth (06:00:00 - 20:59:00, 1-min frequency)\n",
    "        chunk = chunk[\n",
    "                (chunk.xd_id.isin(new_in_xd_int)) &\n",
    "                (start_date <= chunk.measurement_tstamp) & \n",
    "                (chunk.measurement_tstamp < end_date) & \n",
    "                (chunk.measurement_tstamp.dt.hour*60 + chunk.measurement_tstamp.dt.minute >= 330) & \n",
    "                (chunk.measurement_tstamp.dt.hour*60 + chunk.measurement_tstamp.dt.minute < 1260) \n",
    "                ]\n",
    "        chunklist.append(chunk)\n",
    "\n",
    "# concat dataframe chunks and merge into one final dataframe \n",
    "downsampled_xd = pd.concat(chunklist) \n",
    "downsampled_xd = downsampled_xd.reset_index(drop=True)  # reset index\n",
    "\n",
    "# save downsampled xd data\n",
    "downsampled_xd.to_csv(\"data/downsampled_xd_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_xd = pd.read_csv(\"data/downsampled_xd_data.csv\")\n",
    "downsampled_xd.measurement_tstamp = pd.to_datetime(downsampled_xd.measurement_tstamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate New Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Prepare Density Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1.1 TMC Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For now we only use TMC density data, because\n",
    "1. our new input has frequency of 5 min, which matches TMC data\n",
    "2. TMC density input already covers all TMC segments, and XD segments (which have been matched with TMCs): new_out_all_in_tmc is subset of new_in_tmc \n",
    "3. XD density comes with 1-min frequency, and has different meaning than \n",
    "\n",
    "'''\n",
    "density_tmc = tmc_all.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"data_density\"]]\n",
    "density_tmc = density_tmc.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"data_density\")\n",
    "\n",
    "# select 233 tmc segments based on old tmc input segments. 233 tmc segments include all new_out_all_in_tmc\n",
    "density_tmc = density_tmc.loc[:, [c for c in density_tmc.columns if c in new_in_tmc]] \n",
    "\n",
    "# fill NAN with \"A\", which denotes \"Fewer than five values\"\n",
    "density_tmc = density_tmc.fillna(\"A\")\n",
    "\n",
    "# convert index to datetime object, and select 29520 rows of interest\n",
    "density_tmc.index = pd.to_datetime(density_tmc.index)\n",
    "density_tmc = density_tmc.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "density_tmc = density_tmc[(density_tmc.index.hour * 60 + density_tmc.index.minute >= 330 ) & (density_tmc.index.hour * 60 + density_tmc.index.minute <= 1225)]  # (29520, 233)\n",
    "\n",
    "# ordinal embedding \n",
    "density_tmc = density_tmc.replace([\"A\", \"B\", \"C\"], [1/6, 3/6, 5/6]) # (29520, 233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1.2 XD Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_xd = downsampled_xd.pivot(index = \"measurement_tstamp\", columns = \"xd_id\", values = \"confidence_score\")  # (152520, 80) = (164 * 930, 80)\n",
    "\n",
    "# select time range\n",
    "density_xd = density_xd[(density_xd.index.hour*60 + density_xd.index.minute >= 330) & \n",
    "                (density_xd.index.hour*60 + density_xd.index.minute < 1230) \n",
    "                ].reset_index()  # (147600, 80)\n",
    "\n",
    "# fill null\n",
    "density_xd = density_xd.fillna(10.)\n",
    "\n",
    "# ordinal embedding\n",
    "density_xd = density_xd.replace([10., 20., 30.], [1/6, 3/6, 5/6])\n",
    "\n",
    "# aggregation\n",
    "density_xd = density_xd.groupby(density_xd.index // 5).mean()  # (29520, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Prepare Speed Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2.1 TMC Speed Feature (All, Truck, Personal Vehicle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - Here I will first extract speed data from 05:30:00~20:55:00, and then extract input feature (05:30:00~20:25:00) and output ground truth (06:00:00~20:55:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed of all vehicles\n",
    "all_spd = tmc_all.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"speed\"]]\n",
    "all_spd = all_spd.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"speed\")\n",
    "\n",
    "# select 233 tmc segments based on old tmc input segments\n",
    "all_spd = all_spd.loc[:, [c for c in all_spd.columns if c in new_in_tmc]]  \n",
    "\n",
    "# convert index to datetime object, and select 29520 rows of interest\n",
    "all_spd.index = pd.to_datetime(all_spd.index)\n",
    "all_spd = all_spd.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "all_spd = all_spd[(all_spd.index.hour * 60 + all_spd.index.minute >= 330 ) & (all_spd.index.hour * 60 + all_spd.index.minute <= 1255)] # (30504, 233) covers 05:30:00~20:55:00, 1961365 NaN\n",
    "\n",
    "all_avg = tmc_all.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"average_speed\"]]\n",
    "all_avg = all_avg.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"average_speed\")\n",
    "all_avg = all_avg.loc[:, [c for c in all_avg.columns if c in new_in_tmc]]  \n",
    "all_avg.index = pd.to_datetime(all_avg.index)\n",
    "all_avg = all_avg.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "all_avg = all_avg[(all_avg.index.hour * 60 + all_avg.index.minute >= 330 ) & (all_avg.index.hour * 60 + all_avg.index.minute <= 1255)] # (30504, 233) 1961415 NaN\n",
    "\n",
    "all_ref = tmc_all.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"reference_speed\"]]\n",
    "all_ref = all_ref.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"reference_speed\")\n",
    "all_ref = all_ref.loc[:, [c for c in all_ref.columns if c in new_in_tmc]] \n",
    "all_ref.index = pd.to_datetime(all_ref.index)\n",
    "all_ref = all_ref.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "all_ref = all_ref[(all_ref.index.hour * 60 + all_ref.index.minute >= 330 ) & (all_ref.index.hour * 60 + all_ref.index.minute <= 1255)] # (30504, 233) 1961365 NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "truck = tmc_truck.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"speed\"]]\n",
    "truck = truck.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"speed\")\n",
    "\n",
    "# select 233 tmc segments based on old tmc input segments\n",
    "truck = truck.loc[:, [c for c in truck.columns if c in new_in_tmc]]  \n",
    "\n",
    "# convert index to datetime object, and select 29520 rows of interest\n",
    "truck.index = pd.to_datetime(truck.index)\n",
    "truck = truck.resample(\"5 min\").asfreq()  # upsampling with 5-min frequency\n",
    "truck = truck.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "truck = truck[(truck.index.hour * 60 + truck.index.minute >= 330 ) & (truck.index.hour * 60 + truck.index.minute <= 1255)] # (30504, 233), 4632499 NaN\n",
    "\n",
    "truck_avg = tmc_truck.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"average_speed\"]]\n",
    "truck_avg = truck_avg.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"average_speed\")\n",
    "truck_avg = truck_avg.loc[:, [c for c in truck_avg.columns if c in new_in_tmc]]  \n",
    "truck_avg.index = pd.to_datetime(truck_avg.index)\n",
    "truck_avg = truck_avg.resample(\"5 min\").asfreq()\n",
    "truck_avg = truck_avg.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "truck_avg = truck_avg[(truck_avg.index.hour * 60 + truck_avg.index.minute >= 330 ) & (truck_avg.index.hour * 60 + truck_avg.index.minute <= 1255)]  # 4632499 NaN\n",
    "\n",
    "truck_ref = tmc_truck.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"reference_speed\"]]\n",
    "truck_ref = truck_ref.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"reference_speed\")\n",
    "truck_ref = truck_ref.loc[:, [c for c in truck_ref.columns if c in new_in_tmc]] \n",
    "truck_ref.index = pd.to_datetime(truck_ref.index)\n",
    "truck_ref = truck_ref.resample(\"5 min\").asfreq()\n",
    "truck_ref = truck_ref.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "truck_ref = truck_ref[(truck_ref.index.hour * 60 + truck_ref.index.minute >= 330 ) & (truck_ref.index.hour * 60 + truck_ref.index.minute <= 1255)]  # 4632499 NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = tmc_pv.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"speed\"]]\n",
    "pv = pv.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"speed\")\n",
    "\n",
    "# select 233 tmc segments based on old tmc input segments\n",
    "pv = pv.loc[:, [c for c in pv.columns if c in new_in_tmc]] \n",
    "\n",
    "# convert index to datetime object, and select 29520 rows of interest\n",
    "pv.index = pd.to_datetime(pv.index)\n",
    "pv = pv.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "pv = pv[(pv.index.hour * 60 + pv.index.minute >= 330 ) & (pv.index.hour * 60 + pv.index.minute <= 1255)]  # (30504, 233), 2382832 NaN\n",
    "\n",
    "pv_avg = tmc_pv.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"average_speed\"]]\n",
    "pv_avg = pv_avg.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"average_speed\")\n",
    "pv_avg = pv_avg.loc[:, [c for c in pv_avg.columns if c in new_in_tmc]]  \n",
    "pv_avg.index = pd.to_datetime(pv_avg.index)\n",
    "pv_avg = pv_avg.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "pv_avg = pv_avg[(pv_avg.index.hour * 60 + pv_avg.index.minute >= 330 ) & (pv_avg.index.hour * 60 + pv_avg.index.minute <= 1255)]  # 2382882 NaN\n",
    "\n",
    "pv_ref = tmc_pv.loc[:, [\"tmc_code\", \"measurement_tstamp\", \"reference_speed\"]]\n",
    "pv_ref = pv_ref.pivot(index = \"measurement_tstamp\", columns = \"tmc_code\", values = \"reference_speed\")\n",
    "pv_ref = pv_ref.loc[:, [c for c in pv_ref.columns if c in new_in_tmc]]  \n",
    "pv_ref.index = pd.to_datetime(pv_ref.index)\n",
    "pv_ref = pv_ref.loc[\"2019-02-10\":\"2019-07-23\"]\n",
    "pv_ref = pv_ref[(pv_ref.index.hour * 60 + pv_ref.index.minute >= 330 ) & (pv_ref.index.hour * 60 + pv_ref.index.minute <= 1255)]  # 2382832 NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with historical and reference speed data\n",
    "all_spd = all_spd.fillna(all_avg)\n",
    "all_spd = all_spd.fillna(all_ref)\n",
    "\n",
    "truck = truck.fillna(truck_avg)\n",
    "truck = truck.fillna(truck_ref)\n",
    "\n",
    "pv = pv.fillna(pv_avg)\n",
    "pv = pv.fillna(pv_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seg_avg = all_spd.mean(axis=0, skipna=True)\n",
    "truck_seg_avg = truck.mean(axis=0, skipna=True)\n",
    "pv_seg_avg = pv.mean(axis=0, skipna=True)\n",
    "\n",
    "# compute fillna weight\n",
    "all_vs_truck = all_seg_avg / truck_seg_avg\n",
    "all_vs_pv = all_seg_avg / pv_seg_avg\n",
    "truck_vs_all = truck_seg_avg / all_seg_avg\n",
    "truck_vs_pv = truck_seg_avg / pv_seg_avg\n",
    "pv_vs_all = pv_seg_avg / all_seg_avg\n",
    "pv_vs_truck = pv_seg_avg / truck_seg_avg\n",
    "\n",
    "# weighted fillna \n",
    "tmc_all = all_spd.fillna(truck * all_vs_truck)  \n",
    "tmc_all = tmc_all.fillna(truck_avg * all_vs_truck)\n",
    "tmc_all = tmc_all.fillna(truck_ref * all_vs_truck)  \n",
    "tmc_all = tmc_all.fillna(pv * all_vs_pv)  \n",
    "tmc_all = tmc_all.fillna(pv_avg * all_vs_pv)\n",
    "tmc_all = tmc_all.fillna(pv_ref * all_vs_pv)\n",
    "\n",
    "tmc_truck = truck.fillna(all_spd * truck_vs_all)  # reduce NAN from 4461384 to 1858961\n",
    "tmc_truck = tmc_truck.fillna(all_avg * truck_vs_all)  \n",
    "tmc_truck = tmc_truck.fillna(all_ref * truck_vs_all)  \n",
    "tmc_truck = tmc_truck.fillna(pv * truck_vs_pv)  \n",
    "tmc_truck = tmc_truck.fillna(pv_avg * truck_vs_pv)\n",
    "tmc_truck = tmc_truck.fillna(pv_ref * truck_vs_pv)\n",
    "\n",
    "tmc_pv = pv.fillna(all_spd * pv_vs_all)  # reduce NaN from 2258550 to 1858961\n",
    "tmc_pv = tmc_pv.fillna(all_avg * pv_vs_all)\n",
    "tmc_pv = tmc_pv.fillna(all_ref * pv_vs_all)\n",
    "tmc_pv = tmc_pv.fillna(truck * pv_vs_truck)\n",
    "tmc_pv = tmc_pv.fillna(truck_avg * pv_vs_truck)\n",
    "tmc_pv = tmc_pv.fillna(truck_ref * pv_vs_truck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate for NAN value\n",
    "tmc_all = tmc_all.interpolate(method=\"linear\")\n",
    "tmc_truck = tmc_truck.interpolate(method=\"linear\")\n",
    "tmc_pv = tmc_pv.interpolate(method = \"linear\")\n",
    "\n",
    "# fill the remaining NaN with column mean\n",
    "tmc_all = tmc_all.fillna(all_seg_avg)\n",
    "tmc_truck = tmc_truck.fillna(truck_seg_avg)\n",
    "tmc_pv = tmc_pv.fillna(pv_seg_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input TMC speed features\n",
    "tmc_all_in = tmc_all[(tmc_all.index.hour * 60 + tmc_all.index.minute >= 330 ) & (tmc_all.index.hour * 60 + tmc_all.index.minute <= 1225)]  #(29520, 233)\n",
    "tmc_truck_in = tmc_truck[(tmc_truck.index.hour * 60 + tmc_truck.index.minute >= 330 ) & (tmc_truck.index.hour * 60 + tmc_truck.index.minute <= 1225)]  #(29520, 233)\n",
    "tmc_pv_in = tmc_pv[(tmc_pv.index.hour * 60 + tmc_pv.index.minute >= 330 ) & (tmc_pv.index.hour * 60 + tmc_pv.index.minute <= 1225)]  #(29520, 233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2.2 XD Speed Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_spd_in = downsampled_xd.pivot(index = \"measurement_tstamp\", columns = \"xd_id\", values = \"speed\")  # (152520, 80), 1-min frequency\n",
    "xd_spd_in = xd_spd_in.interpolate(method=\"linear\") # no NAN\n",
    "\n",
    "# select the appropriate time range for output and input \n",
    "xd_spd_in = xd_spd_in[(xd_spd_in.index.hour*60 + xd_spd_in.index.minute >= 330) & \n",
    "                (xd_spd_in.index.hour*60 + xd_spd_in.index.minute < 1230)].reset_index()   # for input, select 05:30:00 ~ 20:25:00\n",
    "xd_spd_in =  xd_spd_in.groupby(xd_spd_in.index // 5).mean()  # 5-min frequency, (29520, 80), no NAN => used as XD speed input feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Merge New Features into New_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of old_col that will remain as new columns\n",
    "col_idx = []  # will store the indices of 704 old columns that will become new_X (257 tti, 210 sd, 216 inc, 21 weather & time)\n",
    "for i in range(987):\n",
    "    c = old_col[i]\n",
    "    if \"sd\" in c:\n",
    "        c = c[3:]\n",
    "    if \"inc\" in c:\n",
    "        c = c[4:]\n",
    "    \n",
    "    if c in new_in:\n",
    "        col_idx.append(i)\n",
    "col_idx += list(range(987, 1008))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_X without additional features\n",
    "new_X = old_X[:, -1, col_idx] # (29520, 704) 164 days * 180 daily time slots (05:30:00 ~ 20:25:00)\n",
    "\n",
    "# Ordinal Embedding for Inc Features\n",
    "new_X[:,467:683][new_X[:,467:683] == 0.0] = 1/6\n",
    "new_X[:,467:683][new_X[:,467:683] == 1.0] = 3/6\n",
    "new_X[:,467:683][new_X[:,467:683] == 2.0] = 5/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (29520, 1796) \n",
    "# second dimension: \n",
    "#       313 density (233 tmc + 80 xd)\n",
    "#       779 raw speed (80 xd spd, 233 tmc spd all, 233 tmc spd truck, 233 tmc spd pv) \n",
    "#       257 tti\n",
    "#       210 sd\n",
    "#       216 inc\n",
    "#       21 weather & time\n",
    "final_X = np.concatenate((density_tmc.to_numpy(), density_xd.to_numpy(), xd_spd_in.to_numpy(), tmc_all_in.to_numpy(), tmc_truck_in.to_numpy(), tmc_pv_in.to_numpy(), new_X), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Normalization (min-max normalization)\n",
    "scaler = MinMaxScaler()\n",
    "final_X = scaler.fit_transform(final_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/new_X.npy\", final_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Generate New Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Prepare Speed Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1.1 TMC Speed Ground Truth (All, Truck, PV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output TMC speed ground truth\n",
    "tmc_all_out = tmc_all[(tmc_all.index.hour * 60 + tmc_all.index.minute >= 360 ) & (tmc_all.index.hour * 60 + tmc_all.index.minute <= 1255)]  #(29520, 233)\n",
    "tmc_truck_out = tmc_truck[(tmc_truck.index.hour * 60 + tmc_truck.index.minute >= 360 ) & (tmc_truck.index.hour * 60 + tmc_truck.index.minute <= 1255)]  #(29520, 233)\n",
    "tmc_pv_out = tmc_pv[(tmc_pv.index.hour * 60 + tmc_pv.index.minute >= 360 ) & (tmc_pv.index.hour * 60 + tmc_pv.index.minute <= 1255)]  #(29520, 233)\n",
    "\n",
    "tmc_all_out = tmc_all_out[new_out_all_in_tmc]  # (29520, 70)\n",
    "tmc_truck_out = tmc_truck_out[new_out_all_in_tmc]  # (29520, 70)\n",
    "tmc_pv_out = tmc_pv_out[new_out_all_in_tmc]  # (29520, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect outliers to mark as incident\n",
    "tmc_inc = pd.DataFrame(index = tmc_all_out.index,columns = tmc_all_out.columns)\n",
    "\n",
    "# speed entries that are extremely slow (z-score <= -3) are marked as event\n",
    "tmc_inc[stats.zscore(tmc_all_out) <= -3] = 1.0  # 16547 outliers\n",
    "tmc_inc[stats.zscore(tmc_truck_out) <= -3] = 1.0  # 16547 -> 19787\n",
    "tmc_inc[stats.zscore(tmc_pv_out) <= -3] = 1.0  # 19787 -> 22491 outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22491.0, 2043909)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tmc_inc.sum()), sum(tmc_inc.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1.2 XD Speed Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_spd_out = downsampled_xd[downsampled_xd.xd_id.isin(new_out_all_in_xd_int)] \n",
    "xd_spd_out = xd_spd_out.pivot(index = \"measurement_tstamp\", columns = \"xd_id\", values = \"speed\")  # (152520, 69), 1-min frequency\n",
    "xd_spd_out = xd_spd_out.interpolate(method=\"linear\") # no NAN\n",
    "\n",
    "# select the appropriate time range for output and input \n",
    "xd_spd_out = xd_spd_out[(xd_spd_out.index.hour*60 + xd_spd_out.index.minute >= 360) & \n",
    "                (xd_spd_out.index.hour*60 + xd_spd_out.index.minute < 1260) \n",
    "                ]  # for output, select 06:00:00 ~ 20:59:00 => 1-min frequency, (147600, 69), no NAN => used as XD speed output ground truth\n",
    "\n",
    "# rearrange column order, and add the missing column to make 70 columns in total\n",
    "xd_spd_out = xd_spd_out[new_out_all_in_xd_int]  # (147600, 70), same column order as new_out_all_in_xd_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect outliers to mark as incident\n",
    "xd_inc = pd.DataFrame(columns = xd_spd_out.columns, index = xd_spd_out.index)\n",
    "\n",
    "# speed entries that are extremely slow (z-score <= -3) are marked as event\n",
    "xd_inc[stats.zscore(xd_spd_out) <= -3] = 1.0  # 73320 outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73320.0, 10258680)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(xd_inc.sum()), sum(xd_inc.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10258680\n",
      "10239418\n"
     ]
    }
   ],
   "source": [
    "speed_outliers_1_min = xd_inc.copy()\n",
    "speed_outliers_1_min.columns = tmc_inc.columns\n",
    "print(sum(speed_outliers_1_min.isnull().sum()))\n",
    "speed_outliers_1_min = speed_outliers_1_min.add(tmc_inc, fill_value=0.0)\n",
    "print(sum(speed_outliers_1_min.isnull().sum()))\n",
    "speed_outliers_1_min[speed_outliers_1_min>0] = 1.0  # modify duplicate marks to just 1.0, (147600, 70), 92582 outliers in total, 1-min frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92582.0, 10239418)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(speed_outliers_1_min.sum()), sum(speed_outliers_1_min.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_outliers_5_min = speed_outliers_1_min.reset_index(drop=True)\n",
    "speed_outliers_5_min = speed_outliers_5_min.groupby(speed_outliers_5_min.index // 5).mean()  \n",
    "speed_outliers_5_min[speed_outliers_5_min > 0.0] = 1.0  # (29520, 70), 40049 outliers in total, 5-min frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40049.0, 2026351)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(speed_outliers_5_min.sum()), sum(speed_outliers_5_min.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Prepare Incident Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2.1 Load Incident Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_inc_label = pd.read_csv(\"data/incident_labels.csv\")  # 5-min frequency\n",
    "out_inc_label = out_inc_label.set_index(\"measurement_tstamp\")\n",
    "out_inc_label.index = pd.to_datetime(out_inc_label.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2.2 Incident Data with 5-min Frequency (will be paired with TMC speed ground truth to construct new_Y_tmc.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows based on timestamps of our interest (2019.2.10 ~ 2019.7.23, 06:00:00~20:59:00 in 1-min frequency everyday)\n",
    "out_inc_5_min = out_inc_label[\n",
    "                (start_date <= out_inc_label.index) & \n",
    "                (out_inc_label.index < end_date) & \n",
    "                (out_inc_label.index.hour*60 + out_inc_label.index.minute >= 360) & \n",
    "                (out_inc_label.index.hour*60 + out_inc_label.index.minute < 1260) \n",
    "                ]\n",
    "\n",
    "# select columns of new out segments \n",
    "out_inc_5_min = out_inc_5_min.loc[:, new_out_tmc_xd].reset_index(drop=True)  # (29520, 70), 17892 events\n",
    "\n",
    "# rename columns to represent all segments in TMC IDs, so that we can add out_inc_5_min and speed_outliers_5_min\n",
    "out_inc_5_min.columns = speed_outliers_5_min.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_out_inc_5_min = out_inc_5_min.copy()\n",
    "final_out_inc_5_min = final_out_inc_5_min.add(speed_outliers_5_min, fill_value=0.0)\n",
    "final_out_inc_5_min[final_out_inc_5_min>0.0] = 1.0  # (29520, 70), 54048 events, 5-min frequency, has the same columns with tmc speed ground truths (tmc_all_out, tmc_truck_out, tmc_pv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54048.0, 0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_out_inc_5_min.sum()), sum(final_out_inc_5_min.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2.2 Incident Data with 1-min Frequency (Upsampled from one with 5-min frequency, will be paired with XD speed ground truth to construct new_Y_xd.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_inc_1_min = out_inc_label.resample(\"1 min\").asfreq() # upsampling with 1-min frequency\n",
    "\n",
    "# select rows based on timestamps of our interest (2019.2.10 ~ 2019.7.23, 06:00:00~20:59:00 in 1-min frequency everyday)\n",
    "out_inc_1_min = out_inc_1_min[\n",
    "                (start_date <= out_inc_1_min.index) & \n",
    "                (out_inc_1_min.index < end_date) & \n",
    "                (out_inc_1_min.index.hour*60 + out_inc_1_min.index.minute >= 360) & \n",
    "                (out_inc_1_min.index.hour*60 + out_inc_1_min.index.minute < 1260) \n",
    "                ]\n",
    "\n",
    "# select columns of new out segments \n",
    "out_inc_1_min = out_inc_1_min.loc[:, new_out_tmc_xd]  # (147600, 70), 17892 events\n",
    "\n",
    "# rename columns to represent all segments in TMC IDs, so that we can add out_inc_1_min and speed_outliers_1_min\n",
    "out_inc_1_min.columns = speed_outliers_1_min.columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_out_inc_1_min = out_inc_1_min.copy()\n",
    "final_out_inc_1_min = final_out_inc_1_min.add(speed_outliers_1_min, fill_value=0.0)\n",
    "final_out_inc_1_min[final_out_inc_1_min>0.0] = 1.0  # (147600, 70), 106871 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be cautious about the order of backfilling and forward filling\n",
    "# We first do a backfilling because when an incident occurs, we want the segment to be marked \"1\" as early as possible\n",
    "# Backfilling allows NaN slots before a slot already marked \"1\" to be marked as \"1\" as well.\n",
    "# Then, we do a forward filling to fill NaN at the end of the dataframe\n",
    "final_out_inc_1_min = final_out_inc_1_min.fillna(method = \"bfill\")\n",
    "final_out_inc_1_min = final_out_inc_1_min.fillna(method = \"ffill\") # (147600, 70), 257248 events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Merge Speed Data and Incident Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2.1 Output 1 (5-min Frequency) - TMC Speed + Incident Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last dimension:\n",
    "# 1. TMC speed (all)\n",
    "# 2. TMC speed (truck)\n",
    "# 3. TMC speed (personal vehicle)\n",
    "# 4. incident \n",
    "new_Y_tmc = np.stack((tmc_all_out.to_numpy().astype(\"float64\"), tmc_truck_out.to_numpy().astype(\"float64\"), tmc_pv_out.to_numpy().astype(\"float64\"), final_out_inc_5_min.to_numpy().astype(\"float64\")), axis=-1)  # (29520, 70, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2.2 Output 2 (1-min Frequency) - XD Speed + Incident Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last dimension:\n",
    "# 1. xd speed \n",
    "# 2. incident \n",
    "new_Y_xd = np.stack((xd_spd_out.to_numpy().astype(\"float64\"), final_out_inc_1_min.to_numpy().astype(\"float64\")), axis=-1)  # (147600, 70, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/new_Y_tmc.npy\", new_Y_tmc)\n",
    "np.save(\"data/new_Y_xd.npy\", new_Y_xd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
